{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment of Large Vision-Language Models for Radiological Image Analysis\n",
    "\n",
    "Code accompanying this paper:\n",
    "\n",
    "*Strotzer QD, Nieberle F, Kupke LS, Napodano G, Muertz A, Meiler S, Einspieler I, Rennert J, Strotzer M, Wiesinger I, Wendl C, Stroszczynski C, Hamer O, Schicho A (2024). Toward Foundation Models in Radiology? Quantitative Assessment of GPT-4Vâ€™s Multimodal and Multianatomic Region Capabilities. Radiology (in press)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import base64\n",
    "import openai\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH = Path(\"PATH/TO/RAW/IMAGES\")\n",
    "RESULTS_PATH = Path(\"PATH/TO/RESULTS/FOLDER\")\n",
    "\n",
    "files = glob.glob(os.path.join(SOURCE_PATH, \"**\"), recursive=True)\n",
    "print(len(files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_aspect_ratio(img):\n",
    "    \"\"\"\n",
    "    Crops an image to a target aspect ratio of 4:3 (1.33) while maintaining the image's center.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    img : numpy.ndarray\n",
    "        The input image array. Can be a 2D grayscale image or a 3D image with multiple channels.\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    numpy.ndarray\n",
    "        The cropped image array with an aspect ratio of 4:3, or the original image if no cropping is needed.\n",
    "    \"\"\"\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    aspect_ratio = max(width, height) / min(width, height)\n",
    "\n",
    "    if aspect_ratio > 1.33:\n",
    "\n",
    "        if width > height:\n",
    "            new_width = int(height * 1.33)\n",
    "            left = int(np.ceil((width - new_width) / 2))\n",
    "            top = 0\n",
    "            right = left + new_width\n",
    "            bottom = height\n",
    "        else:\n",
    "            new_height = int(width * 1.33)\n",
    "            left = 0\n",
    "            top = int(np.ceil((height - new_height) / 2))\n",
    "            right = width\n",
    "            bottom = top + new_height\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            cropped_img = img[top:bottom, left:right]\n",
    "        else:\n",
    "            cropped_img = img[top:bottom, left:right, ...]\n",
    "    else:\n",
    "        cropped_img = img\n",
    "\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img):\n",
    "    \"\"\"\n",
    "    Resize an image to a maximum dimension of 768 pixels while preserving the aspect ratio.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    img : numpy.ndarray\n",
    "        The input image as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    numpy.ndarray\n",
    "        The resized image with the new dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    if width < height:\n",
    "        new_width = 768\n",
    "        scale_ratio = new_width / width\n",
    "        new_height = int(height * scale_ratio)\n",
    "    else:\n",
    "        new_height = 768\n",
    "        scale_ratio = new_height / height\n",
    "        new_width = int(width * scale_ratio)\n",
    "\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (new_width, new_height),\n",
    "        interpolation=cv2.INTER_AREA,\n",
    "    )\n",
    "\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize_and_rescale(image):\n",
    "    \"\"\"\n",
    "    Perform Z-score normalization on a grayscale image and rescale it to 0-255.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    image : numpy.ndarray\n",
    "        A 2D numpy array representing a grayscale image.\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    numpy.ndarray\n",
    "        A 2D numpy array of the rescaled normalized image.\n",
    "    \"\"\"\n",
    "\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "\n",
    "    normalized_image = (image - mean) / std\n",
    "\n",
    "    min_val = normalized_image.min()\n",
    "    max_val = normalized_image.max()\n",
    "    rescaled_image = 255 * (normalized_image - min_val) / (max_val - min_val)\n",
    "\n",
    "    rescaled_image = rescaled_image.astype(np.uint8)\n",
    "\n",
    "    return rescaled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_stats_and_show(img):\n",
    "    \"\"\"\n",
    "    Helper function for testing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"type\", type(img))\n",
    "    print(\"shape\", img.shape)\n",
    "    print(\"min\", np.min(img))\n",
    "    print(\"max\", np.max(img))\n",
    "    print(\"mean\", np.mean(img))\n",
    "    print(\"std\", np.std(img))\n",
    "\n",
    "    plt.imshow(img, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(img, bins='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(files):\n",
    "    img = cv2.imread(str(file), 0)\n",
    "    \n",
    "    cropped_img = crop_to_aspect_ratio(img)\n",
    "\n",
    "    resized_img = resize_image(cropped_img)\n",
    "\n",
    "    normalized_img = z_score_normalize_and_rescale(resized_img)\n",
    "\n",
    "    cv2.imwrite(os.path.join(RESULTS_PATH, f\"{Path(file).stem}.png\"), normalized_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "- https://platform.openai.com/docs/guides/vision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"PLACE_YOUR_API_KEY_HERE\"\n",
    "\n",
    "CLIENT = openai.OpenAI(api_key=API_KEY)\n",
    "\n",
    "SYSTEM_PROMPT = \"PLACE_SYSTEM_PROMPT_HERE\"\n",
    "\n",
    "MODEL_NAME = \"MODEL_NAME\"\n",
    "\n",
    "files = glob.glob(os.path.join(RESULTS_PATH, \"**\", \"*.png\"), recursive=True)\n",
    "print(len(files), \"Image files found in\", RESULTS_PATH)\n",
    "\n",
    "os.makedirs(os.path.join(RESULTS_PATH, \"model_output\"), exist_ok=True)\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encodes an image file to a base64-encoded string to be appended to the prompt.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    image_path : str\n",
    "        The file path of the image to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    str\n",
    "        A base64-encoded string representation of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GPT_response(img_path):\n",
    "    \"\"\"\n",
    "    Creates a GPT response using a provided image path.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    img_path : str\n",
    "        The file path of the image to be encoded and sent to the GPT model.\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    dict\n",
    "        The GPT model's response as a dictionary.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "        - Replace \"PROMPT\" with the desired text prompt for the GPT model.\n",
    "        - Set \"SET_NUMBER_OF_MAX_TOKENS_HERE\" to specify the maximum tokens for the response.\n",
    "        - Set \"SET_TEMPERATURE_HERE\" to control the randomness of the output.\n",
    "        - Consider upgrading to the newer \"gpt-4o\" model as the gpt-4-vision-preview will soon be deprecated.\n",
    "    \"\"\"\n",
    "    base64_image = encode_image(img_path)\n",
    "\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",  # soon to be deprecated. Newer model: \"gpt-4o\"\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\",\n",
    "                            \"detail\": \"high\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        stream=False,\n",
    "        max_tokens=\"SET_NUMBER_OF_MAX_TOKENS_HERE\",\n",
    "        temperature=\"SET_TEMPERATURE_HERE\",\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(files): \n",
    "    if not os.path.isfile(\n",
    "        os.path.join(RESULTS_PATH, \"model_output\", f\"{MODEL_NAME}_{Path(file).stem}.txt\")\n",
    "    ):\n",
    "        try:\n",
    "            resp = create_GPT_response(file)\n",
    "\n",
    "            final_string = (\n",
    "                resp.choices[0].message.content\n",
    "                + \";\"\n",
    "                + \"\\n\"\n",
    "                + \"prompt_tokens: \"\n",
    "                + str(resp.usage.prompt_tokens)\n",
    "                + \";\"\n",
    "                + \"\\n\"\n",
    "                + \"response_tokens: \"\n",
    "                + str(resp.usage.completion_tokens)\n",
    "                + \";\"\n",
    "                + \"\\n\"\n",
    "                + \"total_tokens: \"\n",
    "                + str(resp.usage.total_tokens)\n",
    "                + \";\"\n",
    "            )\n",
    "\n",
    "            with open(\n",
    "                os.path.join(RESULTS_PATH, \"model_output\", f\"GPT4-V_{Path(file).stem}.txt\"),\n",
    "                \"w\",\n",
    "            ) as textfile:\n",
    "                textfile.write(final_string)\n",
    "\n",
    "        except openai.error.BadRequestError as e:\n",
    "            print(file, e)\n",
    "\n",
    "    else:\n",
    "        print(\"Report already exists for\", file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table = pd.read_excel(\n",
    "    os.path.join(\n",
    "        SOURCE_PATH, \"EXCEL_TABLE_CONTAINING_IDs.xlsx\"\n",
    "    ),\n",
    "    header=0,\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "txtfiles = glob.glob(\n",
    "    os.path.join(RESULTS_PATH, \"model_output\", \"*.txt\"), recursive=True\n",
    ")\n",
    "\n",
    "case_ids = res_table.index.to_list()\n",
    "\n",
    "for file in txtfiles:\n",
    "    data = Path(file).read_text()\n",
    "\n",
    "    case_id = str(Path(file).stem).replace(MODEL_NAME + \"_\", \"\")\n",
    "\n",
    "    if case_id in case_ids:\n",
    "        res_table.loc[case_id, \"Output_text\"] = data\n",
    "\n",
    "        try:\n",
    "            prompt_tokens = re.findall(r\"prompt_tokens\\s*:\\s*(\\d+);\", data)\n",
    "            res_table.loc[case_id, \"prompt_tokens\"] = int(prompt_tokens[0])\n",
    "\n",
    "            prompt_tokens = re.findall(r\"response_tokens\\s*:\\s*(\\d+);\", data)\n",
    "            res_table.loc[case_id, \"response_tokens\"] = int(prompt_tokens[0])\n",
    "\n",
    "            prompt_tokens = re.findall(r\"total_tokens\\s*:\\s*(\\d+);\", data)\n",
    "            res_table.loc[case_id, \"total_tokens\"] = int(prompt_tokens[0])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e, file)\n",
    "            pass\n",
    "\n",
    "\n",
    "res_table.to_excel(\n",
    "    os.path.join(\n",
    "        RESULTS_PATH, \"OUTPUT_TABLE_NAME.xlsx\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
